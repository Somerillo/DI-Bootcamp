{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Analyzing Confusion Matrix\n",
    "\n",
    "## 1. Definitions of Confusion Matrix Terms in Email Spam Detection\n",
    "\n",
    "- **True Positives (TP)**:\n",
    "  - Emails correctly classified as \"Spam.\"\n",
    "  - Example: An actual spam email is detected as spam by the classifier.\n",
    "\n",
    "- **True Negatives (TN)**:\n",
    "  - Emails correctly classified as \"Not Spam.\"\n",
    "  - Example: A legitimate email is correctly identified as not spam.\n",
    "\n",
    "- **False Positives (FP)**:\n",
    "  - Emails incorrectly classified as \"Spam\" when they are actually \"Not Spam.\"\n",
    "  - Example: A legitimate email from your boss is mistakenly flagged as spam.\n",
    "\n",
    "- **False Negatives (FN)**:\n",
    "  - Emails incorrectly classified as \"Not Spam\" when they are actually \"Spam.\"\n",
    "  - Example: A phishing email is mistakenly marked as not spam and reaches your inbox.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Calculating Metrics\n",
    "\n",
    "Given the confusion matrix values:\n",
    "- **TP** = Number of True Positives\n",
    "- **TN** = Number of True Negatives\n",
    "- **FP** = Number of False Positives\n",
    "- **FN** = Number of False Negatives\n",
    "\n",
    "### Formulas:\n",
    "1. **Accuracy**:\n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "   $$\n",
    "   - Measures the overall correctness of the classifier.\n",
    "\n",
    "2. **Precision**:\n",
    "   $$\n",
    "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "   $$\n",
    "   - Focuses on how many predicted \"Spam\" emails are actually spam.\n",
    "   - High precision means fewer legitimate emails are flagged as spam.\n",
    "\n",
    "3. **Recall (Sensitivity)**:\n",
    "   $$\n",
    "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "   $$\n",
    "   - Measures how many actual spam emails were correctly identified.\n",
    "   - High recall ensures most spam emails are detected.\n",
    "\n",
    "4. **F1-Score**:\n",
    "   $$\n",
    "   F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "   - Harmonic mean of precision and recall, balancing both metrics.\n",
    "\n",
    "### Example Calculation:\n",
    "Assume the confusion matrix values are:\n",
    "- TP = 50, TN = 40, FP = 10, FN = 20\n",
    "\n",
    "1. Accuracy:\n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{50 + 40}{50 + 40 + 10 + 20} = \\frac{90}{120} = 0.75\n",
    "   $$\n",
    "\n",
    "2. Precision:\n",
    "   $$\n",
    "   \\text{Precision} = \\frac{50}{50 + 10} = \\frac{50}{60} = 0.833\n",
    "   $$\n",
    "\n",
    "3. Recall:\n",
    "   $$\n",
    "   \\text{Recall} = \\frac{50}{50 + 20} = \\frac{50}{70} = 0.714\n",
    "   $$\n",
    "\n",
    "4. F1-Score:\n",
    "   $$\n",
    "   F1 = 2 \\times \\frac{0.833 \\times 0.714}{0.833 + 0.714} = 2 \\times \\frac{0.595}{1.547} = 0.769\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "## 3. Impact of False Positives vs False Negatives\n",
    "\n",
    "### Higher Number of False Positives (FP):\n",
    "- **Impact**:\n",
    "  - Legitimate emails are incorrectly flagged as spam.\n",
    "  - This can frustrate users, especially if important emails (e.g., work-related) are sent to the spam folder.\n",
    "- **Metrics Affected**:\n",
    "  - Precision decreases because more non-spam emails are misclassified as spam.\n",
    "    - Lower precision means a higher proportion of flagged emails are not actually spam.\n",
    "  \n",
    "### Higher Number of False Negatives (FN):\n",
    "- **Impact**:\n",
    "  - Spam emails are incorrectly classified as not spam and reach the inbox.\n",
    "  - This can pose security risks, such as phishing attacks or malware delivery.\n",
    "- **Metrics Affected**:\n",
    "  - Recall decreases because fewer actual spam emails are detected.\n",
    "    - Lower recall means the classifier fails to catch a significant portion of spam.\n",
    "\n",
    "### Trade-Off Between FP and FN:\n",
    "- The balance between FP and FN depends on the use case:\n",
    "  - In email spam detection, minimizing FN might be prioritized to ensure harmful emails do not reach users' inboxes, even at the cost of more FP (i.e., stricter filters).\n",
    "  - However, excessive FP can reduce user trust in the system, so a balance is necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: Evaluating Trade-offs in Metrics\n",
    "\n",
    "## 1. **Why High Recall is More Important in Medical Diagnosis**\n",
    "In a medical diagnosis context, recall (sensitivity) measures the ability of the model to correctly identify all patients with the disease. High recall is critical because:\n",
    "- Missing a true positive (false negative) means failing to detect a patient who actually has the disease, which could lead to severe health consequences or even death.\n",
    "- False positives (incorrectly diagnosing a healthy person as diseased) are less harmful because they typically result in additional testing or follow-up, which is less risky than missing a diagnosis.\n",
    "\n",
    "For example, in cancer detection, it is far more critical to identify all potential cancer cases (even if some healthy individuals are flagged) than to risk leaving undiagnosed patients untreated.\n",
    "\n",
    "\n",
    "\n",
    "## 2. **Scenario Where Precision Becomes More Important**\n",
    "Precision becomes more important in scenarios where false positives have significant consequences. For instance:\n",
    "- **Fraud Detection**: Flagging legitimate transactions as fraudulent (false positives) can frustrate customers and damage trust in the system. Here, high precision ensures that only actual fraudulent transactions are flagged.\n",
    "- **Spam Email Filtering**: Marking important legitimate emails as spam (false positives) can lead to missed opportunities or critical communication failures. High precision minimizes these errors.\n",
    "\n",
    "In these cases, it is acceptable to miss some true positives (low recall) if it ensures that flagged instances are highly reliable.\n",
    "\n",
    "\n",
    "\n",
    "## 3. **Consequences of Solely Focusing on Accuracy in Imbalanced Datasets**\n",
    "Accuracy measures the overall correctness of predictions but can be misleading in imbalanced datasets, where one class significantly outweighs the other. For example:\n",
    "- In a dataset where 95% of patients are healthy and only 5% have a disease, a model that predicts \"healthy\" for every case achieves 95% accuracy but fails to detect any diseased patients (0 recall for the diseased class).\n",
    "\n",
    "### Potential Consequences:\n",
    "1. **False Sense of Model Performance**:\n",
    "   - High accuracy may mask poor performance on the minority class (e.g., diseased patients), leading to dangerous real-world outcomes.\n",
    "   \n",
    "2. **Neglecting Minority Class**:\n",
    "   - The model might prioritize majority class predictions, ignoring the minority class entirely.\n",
    "   \n",
    "3. **Unethical Outcomes**:\n",
    "   - In medical or safety-critical applications, focusing solely on accuracy could result in harm to individuals in the minority class.\n",
    "\n",
    "### Mitigation Strategies:\n",
    "- Use metrics like **F1-score**, which balances precision and recall.\n",
    "- Evaluate performance with class-specific metrics like recall for minority classes.\n",
    "- Consider using weighted accuracy or cost-sensitive learning to account for imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: Understanding Cross-Validation and Learning Curves\n",
    "\n",
    "## 1. **Difference Between K-Fold Cross-Validation and Stratified K-Fold Cross-Validation**\n",
    "\n",
    "### K-Fold Cross-Validation:\n",
    "- Divides the dataset into $ k $ equally sized folds.\n",
    "- The model is trained on $ k-1 $ folds and tested on the remaining fold, iterating $ k $ times so that each fold is used as a test set once.\n",
    "- Suitable for datasets where class distribution is not a concern.\n",
    "\n",
    "### Stratified K-Fold Cross-Validation:\n",
    "- Ensures that each fold maintains the same class distribution as the original dataset.\n",
    "- Particularly useful for imbalanced datasets (e.g., when one class is significantly underrepresented).\n",
    "  \n",
    "### Choice for Housing Price Prediction:\n",
    "- **K-Fold Cross-Validation** would be preferred because predicting housing prices is a regression task, where maintaining class distribution is not relevant. Stratification is more applicable to classification problems.\n",
    "\n",
    "\n",
    "\n",
    "## 2. **Learning Curves and Their Use**\n",
    "\n",
    "### What Are Learning Curves?\n",
    "- **Definition**: Graphical representations of model performance (e.g., loss or accuracy) on training and validation sets as a function of training iterations or dataset size.\n",
    "- **Purpose**: Diagnose model behavior, such as underfitting, overfitting, or good fit.\n",
    "\n",
    "### How They Help:\n",
    "1. **Diagnose Underfitting**:\n",
    "   - Training and validation errors are both high and do not improve with more training data or iterations.\n",
    "   - Indicates the model is too simple or lacks capacity to capture the patterns in the data.\n",
    "\n",
    "2. **Diagnose Overfitting**:\n",
    "   - Training error decreases significantly, but validation error increases after a point.\n",
    "   - Indicates the model is memorizing training data instead of generalizing.\n",
    "\n",
    "3. **Assess Data Sufficiency**:\n",
    "   - If validation error decreases with more data, adding more training samples may improve performance.\n",
    "\n",
    "\n",
    "\n",
    "## 3. **Implications of Underfitting and Overfitting**\n",
    "\n",
    "### Underfitting:\n",
    "- **Symptoms on Learning Curve**:\n",
    "  - Training loss remains high and flat.\n",
    "  - Validation loss mirrors training loss with no significant improvement.\n",
    "- **Causes**:\n",
    "  - Model complexity is too low (e.g., insufficient features or too simple algorithm).\n",
    "  - Insufficient training time or poor hyperparameter tuning.\n",
    "- **Solutions**:\n",
    "  - Use a more complex model (e.g., increase number of features or layers).\n",
    "  - Train for more epochs or adjust learning rate.\n",
    "  - Add relevant features to better capture patterns in the data.\n",
    "\n",
    "### Overfitting:\n",
    "- **Symptoms on Learning Curve**:\n",
    "  - Training loss decreases significantly but validation loss increases after a certain point.\n",
    "  - Large gap between training and validation losses.\n",
    "- **Causes**:\n",
    "  - Model complexity is too high (e.g., too many parameters or layers).\n",
    "  - Insufficient regularization or noisy training data.\n",
    "- **Solutions**:\n",
    "  - Apply regularization techniques (e.g., L1/L2 penalties, dropout).\n",
    "  - Use early stopping to halt training before overfitting occurs.\n",
    "  - Reduce model complexity by simplifying architecture or pruning features.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "By leveraging cross-validation to evaluate generalization and learning curves to diagnose model behavior, you can iteratively refine your housing price prediction model to achieve an optimal balance between bias (underfitting) and variance (overfitting). This ensures robust performance on unseen data while avoiding pitfalls like overtraining or insufficient learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 4: Impact of Class Imbalance on Model Evaluation\n",
    "\n",
    "## 1. Why Using Accuracy Might Be Misleading\n",
    "- **Definition**: Accuracy measures the proportion of correct predictions (both true positives and true negatives) out of all predictions.\n",
    "- **Problem in Imbalanced Datasets**:\n",
    "  - In a dataset where only 2% of instances are positive (diseased), a model that predicts \"Not Diseased\" for every instance achieves 98% accuracy, despite failing to identify any actual diseased cases (0% recall for the positive class).\n",
    "  - This high accuracy is misleading because it reflects the dominance of the majority class (negative cases) rather than the model's ability to detect rare positive cases.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Importance of Precision and Recall\n",
    "\n",
    "### **Precision**:\n",
    "- Measures the proportion of correctly identified positive cases out of all predicted positives.\n",
    "- **Importance**:\n",
    "  - High precision ensures that most predicted positive cases are truly diseased, reducing false positives.\n",
    "  - In medical diagnosis, false positives might lead to unnecessary tests or treatments, which can be costly or stressful for patients.\n",
    "\n",
    "### **Recall (Sensitivity)**:\n",
    "- Measures the proportion of actual positive cases that are correctly identified.\n",
    "- **Importance**:\n",
    "  - High recall ensures that most diseased patients are correctly detected, minimizing false negatives.\n",
    "  - Missing a true positive (false negative) could result in undiagnosed and untreated diseases, potentially leading to severe health consequences.\n",
    "\n",
    "### Trade-Off Between Precision and Recall:\n",
    "- In this context, **recall is more critical** because failing to identify diseased patients is far more harmful than subjecting healthy individuals to additional tests (false positives).\n",
    "\n",
    "\n",
    "\n",
    "## 3. Strategies to Evaluate and Improve Performance\n",
    "\n",
    "### **Evaluation Strategies**\n",
    "1. **Precision-Recall Curve**:\n",
    "   - Plot precision vs. recall across different classification thresholds.\n",
    "   - The area under the precision-recall curve (PR-AUC) is more informative than ROC-AUC in imbalanced datasets as it focuses on the minority class performance[1][6].\n",
    "\n",
    "2. **F1-Score**:\n",
    "   - Combines precision and recall into a single metric using their harmonic mean:\n",
    "     $$\n",
    "     F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "     $$\n",
    "   - Useful for balancing false positives and false negatives when both metrics are important.\n",
    "\n",
    "3. **Confusion Matrix Analysis**:\n",
    "   - Examine TP, TN, FP, and FN values directly to understand how well the model performs on each class.\n",
    "\n",
    "4. **Class-Specific Metrics**:\n",
    "   - Evaluate metrics like recall specifically for the minority class to ensure it is being detected effectively.\n",
    "\n",
    "\n",
    "\n",
    "### **Improvement Strategies**\n",
    "1. **Resampling Techniques**:\n",
    "   - **Oversampling**: Duplicate or synthetically generate samples from the minority class (e.g., SMOTE).\n",
    "   - **Undersampling**: Reduce samples from the majority class to balance the dataset.\n",
    "\n",
    "2. **Cost-Sensitive Learning**:\n",
    "   - Assign higher misclassification costs to false negatives compared to false positives.\n",
    "   - Train models with class weights to penalize errors on the minority class more heavily.\n",
    "\n",
    "3. **Algorithmic Adjustments**:\n",
    "   - Use specialized models designed for imbalanced data, such as XGBoost or Random Forest with balanced class weights.\n",
    "   - Adjust classification thresholds to prioritize recall over precision.\n",
    "\n",
    "4. **Anomaly Detection Approach**:\n",
    "   - Treat rare disease detection as an anomaly detection problem where minority cases are flagged as anomalies.\n",
    "\n",
    "5. **Data Augmentation**:\n",
    "   - Generate synthetic data points for the minority class using techniques like GANs (Generative Adversarial Networks).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "In highly imbalanced datasets like rare disease detection, accuracy is not a reliable metric due to its bias toward the majority class. Instead, metrics like precision, recall, F1-score, and PR-AUC should be prioritized. Strategies such as resampling, cost-sensitive learning, and threshold adjustments can help improve model performance while ensuring that critical cases in the minority class are not overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 5: Role of Threshold Tuning in Classification Models\n",
    "\n",
    "## 1. **Effect of Changing the Threshold on Precision and Recall**\n",
    "\n",
    "- **Threshold**: The probability score above which a case is classified as \"positive\" (loan default).\n",
    "- **Impact of Increasing Threshold from 0.5 to 0.7**:\n",
    "  - **Precision**:\n",
    "    - Precision is likely to increase because fewer cases will be classified as \"default\" (positive), and those that are classified will have a higher probability of actually being defaults.\n",
    "    - This reduces false positives, improving the proportion of true positives among predicted positives.\n",
    "  - **Recall**:\n",
    "    - Recall is likely to decrease because some true positive cases (actual defaults) with probabilities between 0.5 and 0.7 will now be missed.\n",
    "    - This increases false negatives, reducing the proportion of actual defaults that are correctly identified.\n",
    "\n",
    "### Trade-Off:\n",
    "- Increasing the threshold improves precision at the cost of recall, while lowering the threshold does the opposite.\n",
    "\n",
    "\n",
    "\n",
    "## 2. **Consequences of Setting the Threshold Too High or Too Low**\n",
    "\n",
    "### **Threshold Too High (e.g., 0.9)**:\n",
    "- **Consequences**:\n",
    "  - Very few cases will be classified as \"default.\"\n",
    "  - High precision but very low recall.\n",
    "  - Many actual defaulters (false negatives) will go undetected, leading to significant financial losses for the bank as these customers are incorrectly approved for loans.\n",
    "\n",
    "### **Threshold Too Low (e.g., 0.3)**:\n",
    "- **Consequences**:\n",
    "  - Many cases will be classified as \"default.\"\n",
    "  - High recall but low precision.\n",
    "  - A large number of non-defaulters (false positives) will be flagged as defaulters, potentially resulting in unnecessary loan rejections or higher interest rates for reliable clients, damaging customer satisfaction and trust.\n",
    "\n",
    "\n",
    "\n",
    "## 3. **Using ROC Curves and AUC to Find the Optimal Threshold**\n",
    "\n",
    "### **ROC Curve**:\n",
    "- A plot of the True Positive Rate (TPR or Recall) vs. False Positive Rate (FPR) at various thresholds.\n",
    "- Helps visualize the trade-off between recall and false positives as you adjust the threshold.\n",
    "- Each point on the curve corresponds to a specific threshold.\n",
    "\n",
    "### **AUC (Area Under the Curve)**:\n",
    "- A single scalar value summarizing the ROC curve.\n",
    "- Ranges from 0 to 1, where:\n",
    "  - AUC = 1 indicates perfect classification.\n",
    "  - AUC = 0.5 indicates random guessing.\n",
    "- Higher AUC values indicate better model performance across all thresholds.\n",
    "\n",
    "### **Finding the Optimal Threshold**:\n",
    "1. Analyze the ROC curve to identify a threshold that provides a good balance between TPR and FPR.\n",
    "2. Consider business-specific priorities:\n",
    "   - If minimizing false negatives (recall) is critical, choose a threshold closer to the left side of the ROC curve.\n",
    "   - If minimizing false positives (precision) is critical, choose a threshold closer to the right side of the curve.\n",
    "3. Use metrics like F1-score or a cost-benefit analysis to determine an optimal trade-off point based on business goals.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "Threshold tuning is crucial for balancing precision and recall in binary classification tasks like loan default prediction. While increasing the threshold improves precision at the expense of recall, lowering it does the opposite. ROC curves and AUC provide valuable tools for understanding model performance across thresholds and selecting an optimal threshold based on business priorities, such as minimizing financial losses or maintaining customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Devs_Institute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
