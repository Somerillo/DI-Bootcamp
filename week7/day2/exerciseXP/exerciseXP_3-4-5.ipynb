{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 3 : Scrape Dynamic Content from Rotten Tomatoes\n",
    "Task:\n",
    "\n",
    "-    Use Selenium to navigate to the Rotten Tomatoes Certified Fresh Movies page.\n",
    "-    Extract the HTML content after itâ€™s fully loaded.\n",
    "-    Use BeautifulSoup to parse and extract the movie titles, scores, and release dates.\n",
    "\n",
    "Instructions\n",
    "\n",
    "-    Set up Selenium WebDriver and navigate to the Rotten Tomatoes page.\n",
    "-    Extract the HTML content using driver.page_source.\n",
    "-    Parse the HTML with BeautifulSoup.\n",
    "-    Find and extract the desired movie information.\n",
    "-    Print the extracted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessed rotten tomatoes and content loaded\n",
      "Scrolling completed. Scrolled 0 times.\n",
      "Title: Chasing Chasing Amy\n",
      "Release Date: Streaming Dec 17, 2024\n",
      "Critics score: 94%, public score N/A\n",
      "---\n",
      "Title: Carry-On\n",
      "Release Date: Streaming Dec 13, 2024\n",
      "Critics score: 87%, public score 59%\n",
      "---\n",
      "Title: Heretic\n",
      "Release Date: Streaming Dec 10, 2024\n",
      "Critics score: 91%, public score 76%\n",
      "---\n",
      "Title: Juror #2\n",
      "Release Date: Streaming Dec 3, 2024\n",
      "Critics score: 94%, public score 91%\n",
      "---\n",
      "Title: Conclave\n",
      "Release Date: Streaming Nov 26, 2024\n",
      "Critics score: 93%, public score 86%\n",
      "---\n",
      "Title: Maria\n",
      "Release Date: Streaming Dec 11, 2024\n",
      "Critics score: 74%, public score 70%\n",
      "---\n",
      "Title: The Substance\n",
      "Release Date: Streaming Oct 31, 2024\n",
      "Critics score: 90%, public score 75%\n",
      "---\n",
      "Title: 28 Days Later\n",
      "Release Date: Streaming May 10, 2014\n",
      "Critics score: 87%, public score 85%\n",
      "---\n",
      "Title: Joy\n",
      "Release Date: Streaming Nov 22, 2024\n",
      "Critics score: 90%, public score 89%\n",
      "---\n",
      "Title: Emilia PÃ©rez\n",
      "Release Date: Streaming Nov 13, 2024\n",
      "Critics score: 76%, public score 73%\n",
      "---\n",
      "Title: Interstellar\n",
      "Release Date: Streaming May 24, 2016\n",
      "Critics score: 73%, public score 87%\n",
      "---\n",
      "Title: Sugarcane\n",
      "Release Date: Streaming Dec 10, 2024\n",
      "Critics score: 100%, public score 84%\n",
      "---\n",
      "Title: Christmas Eve in Miller's Point\n",
      "Release Date: Streaming Dec 3, 2024\n",
      "Critics score: 78%, public score 35%\n",
      "---\n",
      "Title: The Holdovers\n",
      "Release Date: Streaming Nov 28, 2023\n",
      "Critics score: 97%, public score 92%\n",
      "---\n",
      "Title: Smile 2\n",
      "Release Date: Streaming Nov 19, 2024\n",
      "Critics score: 86%, public score 81%\n",
      "---\n",
      "Title: Speak No Evil\n",
      "Release Date: Streaming Oct 1, 2024\n",
      "Critics score: 83%, public score 84%\n",
      "---\n",
      "Title: Beetlejuice Beetlejuice\n",
      "Release Date: Streaming Oct 8, 2024\n",
      "Critics score: 76%, public score 79%\n",
      "---\n",
      "Title: The Wild Robot\n",
      "Release Date: Streaming Oct 15, 2024\n",
      "Critics score: 97%, public score 98%\n",
      "---\n",
      "Title: Tangerine\n",
      "Release Date: Streaming Nov 10, 2016\n",
      "Critics score: 96%, public score 75%\n",
      "---\n",
      "Title: Alien: Romulus\n",
      "Release Date: Streaming Oct 15, 2024\n",
      "Critics score: 80%, public score 85%\n",
      "---\n",
      "Title: Klaus\n",
      "Release Date: Streaming Nov 15, 2019\n",
      "Critics score: 95%, public score 96%\n",
      "---\n",
      "Title: Deadpool & Wolverine\n",
      "Release Date: Streaming Oct 1, 2024\n",
      "Critics score: 78%, public score 94%\n",
      "---\n",
      "Title: We Live in Time\n",
      "Release Date: Streaming Nov 22, 2024\n",
      "Critics score: 78%, public score 83%\n",
      "---\n",
      "Title: Longlegs\n",
      "Release Date: Streaming Aug 23, 2024\n",
      "Critics score: 86%, public score 61%\n",
      "---\n",
      "Title: The Bikeriders\n",
      "Release Date: Streaming Jul 9, 2024\n",
      "Critics score: 80%, public score 74%\n",
      "---\n",
      "Title: Gladiator\n",
      "Release Date: Streaming Jun 15, 2011\n",
      "Critics score: 80%, public score 87%\n",
      "---\n",
      "Title: Hit Man\n",
      "Release Date: Streaming Jun 7, 2024\n",
      "Critics score: 95%, public score 92%\n",
      "---\n",
      "Title: Terrifier 3\n",
      "Release Date: Streaming Nov 26, 2024\n",
      "Critics score: 77%, public score 85%\n",
      "---\n",
      "Browser closed successfully.\n"
     ]
    }
   ],
   "source": [
    "# setup seleium webrdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")  # Open browser in maximized mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# navigate to rotten tomatoes webp\n",
    "url = \"https://www.rottentomatoes.com/browse/movies_at_home/critics:certified_fresh\"\n",
    "driver.get(url)\n",
    "\n",
    "# wait for the content to load\n",
    "wait = WebDriverWait(driver, 30)\n",
    "wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".flex-container\")))\n",
    "\n",
    "# connection confirmation\n",
    "print(\"accessed rotten tomatoes and content loaded\")\n",
    "\n",
    "\n",
    "# scroll to load more content\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "scroll_count = 0\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # # need time to load\n",
    "    # time.sleep(4)\n",
    "\n",
    "    # wait for all elements to load\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".flex-container\")))\n",
    "    except TimeoutException:\n",
    "        print(\"No new content loaded.\")\n",
    "        break\n",
    "\n",
    "    # if the height didnt change break out\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    scroll_count += 1\n",
    "\n",
    "# scroll confirmation\n",
    "print(f\"Scrolling completed. Scrolled {scroll_count} times.\")\n",
    "\n",
    "\n",
    "\n",
    "# extract the html content after scrolling\n",
    "html_content = driver.page_source\n",
    "\n",
    "# use beautifulsoup to parse the html\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# find all movie containers\n",
    "movie_containers = soup.select(\".flex-container\")\n",
    "\n",
    "# extract movie information\n",
    "for container in movie_containers:\n",
    "    # extract title\n",
    "    title = container.select_one(\".p--small\").get_text(strip=True)\n",
    "    \n",
    "    # extract release date\n",
    "    release_date = container.select_one(\".smaller\").get_text(strip=True)\n",
    "    \n",
    "    # extract score\n",
    "    score_element = container.select_one(\"score-pairs-deprecated\")\n",
    "    \n",
    "    # Initialize default values for scores\n",
    "    critics_score = \"N/A\"\n",
    "    public_score = \"N/A\"\n",
    "\n",
    "    # Extract score element directly from the container\n",
    "    score_element = container.select_one(\"score-pairs-deprecated\")\n",
    "    \n",
    "    if score_element:\n",
    "        # Extract critics score from rt-text with slot=\"criticsScore\"\n",
    "        critics_score_element = score_element.select_one('rt-text[slot=\"criticsScore\"]') # slot!!!!!!!!!!!\n",
    "        if critics_score_element:\n",
    "            critics_score = critics_score_element.get_text(strip=True)\n",
    "\n",
    "        # Extract audience score from rt-text with slot=\"audienceScore\"\n",
    "        audience_score_element = score_element.select_one('rt-text[slot=\"audienceScore\"]')\n",
    "        if audience_score_element:\n",
    "            public_score = audience_score_element.get_text(strip=True)\n",
    "        \n",
    "\n",
    "    \n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Release Date: {release_date}\")\n",
    "    # print(score_element)\n",
    "    print(f\"Critics score: {critics_score}, public score {public_score}\")\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "# close the browser\n",
    "driver.quit()\n",
    "print(\"Browser closed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŒŸ Exercise 4 : Scrape and Categorize News Articles from a JavaScript-Enabled News Site\n",
    "Task:\n",
    "\n",
    "-    Visit this [website](https://www.bbc.com/innovation/technology).\n",
    "-    Scrape news article titles and their publication dates.\n",
    "-    Categorize articles based on their publication month.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "-    Use Selenium to navigate to a specific news section on the website.\n",
    "-    Extract and parse the HTML content that is dynamically loaded via JavaScript.\n",
    "-    Using BeautifulSoup, extract news article titles and publication dates.\n",
    "-    Categorize articles by their publication month (e.g., â€˜Januaryâ€™, â€˜Februaryâ€™, etc.).\n",
    "-    Print the categorized lists of articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed 'main-content'.\n",
      "Retrieved inner HTML of 'main-content'.\n",
      "Parsed HTML content with BeautifulSoup.\n",
      "retrieved 41 articles:\n",
      "data extracted into dataframe: `df_articles`\n"
     ]
    }
   ],
   "source": [
    "# setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")  # Open browser in maximized mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# navigate to BBC Technology page\n",
    "url = \"https://www.bbc.com/innovation/technology\"\n",
    "driver.get(url)\n",
    "\n",
    "# wait for the \"main-content\" element to load\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "try:\n",
    "    # wait for the element with ID \"main-content\" to be present\n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"main-content\")))\n",
    "    print(\"Successfully accessed 'main-content'.\")\n",
    "\n",
    "    # get the html content of \"main-content\"\n",
    "    main_content_html = driver.find_element(By.ID, \"main-content\").get_attribute('innerHTML')\n",
    "    print(\"Retrieved inner HTML of 'main-content'.\")\n",
    "\n",
    "    # parse html content with beautifulsoup\n",
    "    soup = BeautifulSoup(main_content_html, 'html.parser')\n",
    "    print(\"Parsed HTML content with BeautifulSoup.\")\n",
    "\n",
    "    # find ALL articles containerss within the main content area\n",
    "    articles = soup.find_all('div', attrs={\"data-testid\": \"card-text-wrapper\"})\n",
    "\n",
    "    # make lists to hold titles and last updated dates\n",
    "    titles = []\n",
    "    last_updated_dates = []\n",
    "\n",
    "    \n",
    "    # I have not failed. I've just found 10,000 ways that won't work\n",
    "    if articles:\n",
    "        print(f\"retrieved {len(articles)} articles:\")\n",
    "        for article in articles:\n",
    "            # extract headline\n",
    "            headline = article.find('h2', attrs={\"data-testid\": \"card-headline\"})\n",
    "            title = headline.get_text(strip=True) if headline else \"No Title\"\n",
    "            titles.append(title)\n",
    "\n",
    "            # extract last updated date\n",
    "            last_updated = article.find('span', attrs={\"data-testid\": \"card-metadata-lastupdated\"})\n",
    "            last_updated_text = last_updated.get_text(strip=True) if last_updated else \"article of now\"\n",
    "            last_updated_dates.append(last_updated_text)\n",
    "\n",
    "        # make a df with the extracted data\n",
    "        df_articles = pd.DataFrame({\"title\": titles, \"last updated\": last_updated_dates})\n",
    "        print(\"data extracted into dataframe: `df_articles`\")\n",
    "        # print(df_articles)  # print results\n",
    "    else:\n",
    "        print(\"NOPE, no articles found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"an error occurred: {e}\")\n",
    "\n",
    "\n",
    "# shut the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have to make a delta time function then group by month\n",
      "the delta time funciton comes after a .strip(), to check if its hours or days\n",
      "this exercise is teking way too long and gonna leave like that for the moment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>last updated</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why Final Fantasy director almost rejected his...</td>\n",
       "      <td>article of now</td>\n",
       "      <td>2024-12-16 23:46:33.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telegram pushes extremist groups to users - study</td>\n",
       "      <td>5 hrs ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social media given 'last chance' to tackle ill...</td>\n",
       "      <td>10 hrs ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bitcoin hits new record high of more than $106...</td>\n",
       "      <td>11 hrs ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hospitals to use AI to cut patient visits</td>\n",
       "      <td>16 hrs ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why we become better friends as we age</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Old hospital gowns made into octopus IV drip h...</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What we've learned since the biggest oil spill...</td>\n",
       "      <td>article of now</td>\n",
       "      <td>2024-12-16 23:46:33.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What we know about mysterious drones over New ...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Why 'digital twins' could speed up drug discovery</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Training machines to mimic humans</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The YouTuber who swapped air-traffic control f...</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Musk promises self-driving Tesla taxis, but ar...</td>\n",
       "      <td>12 Oct 2024</td>\n",
       "      <td>2024-10-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tackling digital threats to the US election</td>\n",
       "      <td>5 Oct 2024</td>\n",
       "      <td>2024-10-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Watch: Can BBC reporter's AI clone fool his co...</td>\n",
       "      <td>27 Sep 2024</td>\n",
       "      <td>2024-09-27 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'A tech firm stole our voices - then cloned an...</td>\n",
       "      <td>19 Sep 2024</td>\n",
       "      <td>2024-09-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Miami Heat: The basketball team turned tech st...</td>\n",
       "      <td>7 Sep 2024</td>\n",
       "      <td>2024-09-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Testing out smart swim goggles for the Paris G...</td>\n",
       "      <td>16 Aug 2024</td>\n",
       "      <td>2024-08-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Driverless cars wake US residents with nightti...</td>\n",
       "      <td>15 Aug 2024</td>\n",
       "      <td>2024-08-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>â€˜Pig butcheringâ€™ scammers target BBC reporter</td>\n",
       "      <td>6 Aug 2024</td>\n",
       "      <td>2024-08-06 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>'My boarding pass and luggage tags were filled...</td>\n",
       "      <td>19 Jul 2024</td>\n",
       "      <td>2024-07-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How technology will power the Olympics opening...</td>\n",
       "      <td>18 Jul 2024</td>\n",
       "      <td>2024-07-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Why Olympic venues are using digital twins</td>\n",
       "      <td>17 Jul 2024</td>\n",
       "      <td>2024-07-17 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>UK college students on life without a smartphone</td>\n",
       "      <td>12 Jun 2024</td>\n",
       "      <td>2024-06-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>White gold rush: Harvesting lithium from Great...</td>\n",
       "      <td>13 May 2024</td>\n",
       "      <td>2024-05-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ros Atkins on... How different countries prote...</td>\n",
       "      <td>11 May 2024</td>\n",
       "      <td>2024-05-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Why scientists are making 'moon dust'</td>\n",
       "      <td>10 May 2024</td>\n",
       "      <td>2024-05-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Where do your old mobile phones and TVs go to ...</td>\n",
       "      <td>4 May 2024</td>\n",
       "      <td>2024-05-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Neuralink patient seen playing chess using bra...</td>\n",
       "      <td>21 Mar 2024</td>\n",
       "      <td>2024-03-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>How a US TikTok ban will affect Gen Z</td>\n",
       "      <td>13 Mar 2024</td>\n",
       "      <td>2024-03-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The celebrities 'deepfaked' for US influencer'...</td>\n",
       "      <td>24 Feb 2024</td>\n",
       "      <td>2024-02-24 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Paul Carter tries out Atom Limbs bionic arm</td>\n",
       "      <td>22 Feb 2024</td>\n",
       "      <td>2024-02-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>BBC complains to Apple over misleading shootin...</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Controversial chatbot's safety measures 'a sti...</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mark Zuckerberg's Meta donates $1m to Trump fund</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>WhatsApp and Instagram restored after Meta out...</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Hackers find hole in Krispy Kreme Doughnuts' c...</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>How a long-gone tool built the modern internet</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Chatbot 'encouraged teen to kill parents over ...</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Amazon and eBay to pay 'fair share' for e-wast...</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TikTok asks for emergency pause to looming US ban</td>\n",
       "      <td>7 days ago</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title    last updated  \\\n",
       "0   Why Final Fantasy director almost rejected his...  article of now   \n",
       "1   Telegram pushes extremist groups to users - study       5 hrs ago   \n",
       "2   Social media given 'last chance' to tackle ill...      10 hrs ago   \n",
       "3   Bitcoin hits new record high of more than $106...      11 hrs ago   \n",
       "4           Hospitals to use AI to cut patient visits      16 hrs ago   \n",
       "5              Why we become better friends as we age       1 day ago   \n",
       "6   Old hospital gowns made into octopus IV drip h...      2 days ago   \n",
       "7   What we've learned since the biggest oil spill...  article of now   \n",
       "8   What we know about mysterious drones over New ...       1 day ago   \n",
       "9   Why 'digital twins' could speed up drug discovery      4 days ago   \n",
       "10                  Training machines to mimic humans      4 days ago   \n",
       "11  The YouTuber who swapped air-traffic control f...      4 days ago   \n",
       "12  Musk promises self-driving Tesla taxis, but ar...     12 Oct 2024   \n",
       "13        Tackling digital threats to the US election      5 Oct 2024   \n",
       "14  Watch: Can BBC reporter's AI clone fool his co...     27 Sep 2024   \n",
       "15  'A tech firm stole our voices - then cloned an...     19 Sep 2024   \n",
       "16  Miami Heat: The basketball team turned tech st...      7 Sep 2024   \n",
       "17  Testing out smart swim goggles for the Paris G...     16 Aug 2024   \n",
       "18  Driverless cars wake US residents with nightti...     15 Aug 2024   \n",
       "19      â€˜Pig butcheringâ€™ scammers target BBC reporter      6 Aug 2024   \n",
       "20  'My boarding pass and luggage tags were filled...     19 Jul 2024   \n",
       "21  How technology will power the Olympics opening...     18 Jul 2024   \n",
       "22         Why Olympic venues are using digital twins     17 Jul 2024   \n",
       "23   UK college students on life without a smartphone     12 Jun 2024   \n",
       "24  White gold rush: Harvesting lithium from Great...     13 May 2024   \n",
       "25  Ros Atkins on... How different countries prote...     11 May 2024   \n",
       "26              Why scientists are making 'moon dust'     10 May 2024   \n",
       "27  Where do your old mobile phones and TVs go to ...      4 May 2024   \n",
       "28  Neuralink patient seen playing chess using bra...     21 Mar 2024   \n",
       "29              How a US TikTok ban will affect Gen Z     13 Mar 2024   \n",
       "30  The celebrities 'deepfaked' for US influencer'...     24 Feb 2024   \n",
       "31        Paul Carter tries out Atom Limbs bionic arm     22 Feb 2024   \n",
       "32  BBC complains to Apple over misleading shootin...      3 days ago   \n",
       "33  Controversial chatbot's safety measures 'a sti...      4 days ago   \n",
       "34   Mark Zuckerberg's Meta donates $1m to Trump fund      4 days ago   \n",
       "35  WhatsApp and Instagram restored after Meta out...      5 days ago   \n",
       "36  Hackers find hole in Krispy Kreme Doughnuts' c...      5 days ago   \n",
       "37     How a long-gone tool built the modern internet      5 days ago   \n",
       "38  Chatbot 'encouraged teen to kill parents over ...      5 days ago   \n",
       "39  Amazon and eBay to pay 'fair share' for e-wast...      6 days ago   \n",
       "40  TikTok asks for emergency pause to looming US ban      7 days ago   \n",
       "\n",
       "                          date  \n",
       "0   2024-12-16 23:46:33.839800  \n",
       "1                        delta  \n",
       "2                        delta  \n",
       "3                        delta  \n",
       "4                        delta  \n",
       "5                        delta  \n",
       "6                        delta  \n",
       "7   2024-12-16 23:46:33.839800  \n",
       "8                        delta  \n",
       "9                        delta  \n",
       "10                       delta  \n",
       "11                       delta  \n",
       "12         2024-10-12 00:00:00  \n",
       "13         2024-10-05 00:00:00  \n",
       "14         2024-09-27 00:00:00  \n",
       "15         2024-09-19 00:00:00  \n",
       "16         2024-09-07 00:00:00  \n",
       "17         2024-08-16 00:00:00  \n",
       "18         2024-08-15 00:00:00  \n",
       "19         2024-08-06 00:00:00  \n",
       "20         2024-07-19 00:00:00  \n",
       "21         2024-07-18 00:00:00  \n",
       "22         2024-07-17 00:00:00  \n",
       "23         2024-06-12 00:00:00  \n",
       "24         2024-05-13 00:00:00  \n",
       "25         2024-05-11 00:00:00  \n",
       "26         2024-05-10 00:00:00  \n",
       "27         2024-05-04 00:00:00  \n",
       "28         2024-03-21 00:00:00  \n",
       "29         2024-03-13 00:00:00  \n",
       "30         2024-02-24 00:00:00  \n",
       "31         2024-02-22 00:00:00  \n",
       "32                       delta  \n",
       "33                       delta  \n",
       "34                       delta  \n",
       "35                       delta  \n",
       "36                       delta  \n",
       "37                       delta  \n",
       "38                       delta  \n",
       "39                       delta  \n",
       "40                       delta  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we need to categorize by date\n",
    "df_articles['date'] = None\n",
    "current_time = datetime.now()\n",
    "\n",
    "# Iterate over the 'last updated' column\n",
    "for index, last_updated in enumerate(df_articles['last updated']):\n",
    "    # take the last four characters\n",
    "    year_str = last_updated.strip()[-4:]\n",
    "\n",
    "    try:\n",
    "        # try to convert the last four characters to an integer\n",
    "        year = int(year_str)\n",
    "\n",
    "        # if successful, check if it's greater than 2000\n",
    "        if year > 2000:\n",
    "            # parse the full date string in the format \"DD MMM YYYY\"\n",
    "            full_date = datetime.strptime(last_updated.strip(), \"%d %b %Y\")\n",
    "            df_articles.at[index, 'date'] = full_date  # Assign to 'date' column\n",
    "        else:\n",
    "            df_articles.at[index, 'date'] = \"delta\"  # Assign \"delta\" if year is not greater than 2000\n",
    "\n",
    "    except ValueError:\n",
    "        if last_updated == 'article of now':  # Check against the variable\n",
    "            df_articles.at[index, 'date'] = current_time  # Use assignment operator\n",
    "        else:\n",
    "            df_articles.at[index, 'date'] = 'delta'  # Use assignment operator\n",
    "\n",
    "print('I have to make a delta time function then group by month')\n",
    "print('the delta time funciton comes after a .strip(), to check if its hours or days')\n",
    "print('this exercise is teking way too long and gonna leave like that for the moment')\n",
    "display(df_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 5 : Scrape and Analyze Weather Data from a JavaScript-Enabled Weather Website\n",
    "Task:\n",
    "\n",
    "-    Visit this [website](https://www.accuweather.com/en/us/los-angeles-ca/90012/weather-forecast/348108).\n",
    "-    Scrape weather forecast data including temperature, condition, and humidity.\n",
    "-    Analyze the data to find the average temperature and most common weather condition.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "-    Use Selenium to navigate to the weather forecast page of a specific city.\n",
    "-    Extract and parse the HTML content, focusing on dynamically loaded weather data.\n",
    "-    Using BeautifulSoup, extract relevant weather information like temperature, condition (sunny, cloudy, etc.), and humidity.\n",
    "-    Calculate the average temperature and identify the most common weather condition.\n",
    "-    Print the analysis results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed the current weather card body.\n",
      "Retrieved outer html of the current weather card body.\n",
      "Parsed html content with beautifulsoup for current weather.\n",
      "Successfully accessed the hourly weather list.\n",
      "Retrieved outer html of the hourly weather list.\n",
      "Parsed html content with beautifulsoup for hourly weather.\n"
     ]
    }
   ],
   "source": [
    "# setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")  # Open browser in maximized mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# navigate to BBC Technology page\n",
    "url = \"https://www.accuweather.com/en/us/attica/30607/weather-forecast/2139413\"\n",
    "driver.get(url)\n",
    "\n",
    "# wait for the specific weather card element to load\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "# try:\n",
    "#     # Wait for the element with the specified class to be present\n",
    "#     wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.template-root div.two-column-page-content div.page-column-1 div.page-content.content-module a.cur-con-weather-card.is-desktop.lbar-panel.content-module\")))\n",
    "#     print(\"Successfully accessed the weather card body.\")\n",
    "\n",
    "#     # Get the HTML content of the specified element\n",
    "#     weather_card_body_html = driver.find_element(By.CSS_SELECTOR, \"div.template-root div.two-column-page-content div.page-column-1 div.page-content.content-module a.cur-con-weather-card.is-desktop.lbar-panel.content-module\").get_attribute('outerHTML')\n",
    "#     print(\"Retrieved outer HTML of the weather card body.\")\n",
    "\n",
    "#     # Parse HTML content with BeautifulSoup\n",
    "#     soup = BeautifulSoup(weather_card_body_html, 'html.parser')\n",
    "#     print(\"Parsed HTML content with BeautifulSoup.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # get current weather\n",
    "    # wait for the current weather card element to be present\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.template-root div.two-column-page-content div.page-column-1 div.page-content.content-module a.cur-con-weather-card.is-desktop.lbar-panel.content-module\")))\n",
    "    print(\"Successfully accessed the current weather card body.\")\n",
    "\n",
    "    # get the html content of the current weather card\n",
    "    weather_card_body_html = driver.find_element(By.CSS_SELECTOR, \"div.template-root div.two-column-page-content div.page-column-1 div.page-content.content-module a.cur-con-weather-card.is-desktop.lbar-panel.content-module\").get_attribute('outerHTML')\n",
    "    print(\"Retrieved outer html of the current weather card body.\")\n",
    "\n",
    "    # parse html content with beautifulsoup for current weather\n",
    "    soup_current = BeautifulSoup(weather_card_body_html, 'html.parser')\n",
    "    print(\"Parsed html content with beautifulsoup for current weather.\")\n",
    "\n",
    "\n",
    "    # get hourly weather\n",
    "    # wait for the hourly weather list element to be present\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.hourly-list-wrapper.content-module div.hourly-list.has-arrow-right div.hourly-list__list-wrapper div.hourly-list__list\")))\n",
    "    print(\"Successfully accessed the hourly weather list.\")\n",
    "\n",
    "    # get the html content of the hourly weather list\n",
    "    hourly_list_html = driver.find_element(By.CSS_SELECTOR, \"div.hourly-list-wrapper.content-module div.hourly-list.has-arrow-right div.hourly-list__list-wrapper div.hourly-list__list\").get_attribute('outerHTML')\n",
    "    print(\"Retrieved outer html of the hourly weather list.\")\n",
    "\n",
    "    # parse html content with beautifulsoup for hourly weather\n",
    "    soup_hourly = BeautifulSoup(hourly_list_html, 'html.parser')\n",
    "    print(\"Parsed html content with beautifulsoup for hourly weather.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# shut the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we made one beatiful soup for each element we needed, we get the required data.\n",
    "\n",
    "First we get current conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 13Â°C\n",
      "Weather Condition: Mostly cloudy\n"
     ]
    }
   ],
   "source": [
    "# extracting temp\n",
    "temp_div = soup_current.select_one('div.temp-container > div.temp')\n",
    "if temp_div:\n",
    "    # extract temperature value and unit\n",
    "    temperature = temp_div.get_text(strip=True)\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "else:\n",
    "    print(\"Temperature element not found.\")\n",
    "\n",
    "# extract weather condition\n",
    "condition_span = soup_current.select_one('span.phrase')\n",
    "if condition_span:\n",
    "    # Extract weather condition text\n",
    "    weather_condition = condition_span.get_text(strip=True)\n",
    "    print(f\"Weather Condition: {weather_condition}\")\n",
    "else:\n",
    "    print(\"Weather condition element not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then qe get the average temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup_hourly.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avgerage temperature is 12.5\n"
     ]
    }
   ],
   "source": [
    "# initialize list to store the hourly temps\n",
    "temperature_list = []\n",
    "\n",
    "# find all span elements with the specified class in soup_hourly\n",
    "hourly_time_elements = soup_hourly.find_all('span', class_='hourly-list__list__item-time')\n",
    "\n",
    "# sweep on the hourly time elements\n",
    "for time_element in hourly_time_elements:\n",
    "    temperature_list.append(time_element.text.strip())  # Use .strip() to clean up any extra whitespace\n",
    "\n",
    "# transform string to integer\n",
    "temperature_list = [int(x) for x in temperature_list]\n",
    "\n",
    "# calculate avg temp\n",
    "avg_temp = sum(temperature_list)/len(temperature_list)\n",
    "print(f'the avgerage temperature is {avg_temp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get the most common condition (only icons of conditions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.awxcdn.com/adc-assets/images/weathericons/6.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/38.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/7.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/11.svg', 'https://www.awxcdn.com/adc-assets/images/weathericons/11.svg']\n",
      "the most common weather condition is given by the icon: https://www.awxcdn.com/adc-assets/images/weathericons/7.svg\n"
     ]
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "# initialze list to store the icon urls\n",
    "icon_list = []\n",
    "\n",
    "# find all img elements based on the specified selector in soup_hourly\n",
    "hourly_icon_elements = soup_hourly.select('a.hourly-list__list__item > img:nth-child(2)')\n",
    "\n",
    "# sweep on the horly icons\n",
    "for icon_element in hourly_icon_elements:\n",
    "    icon_src = icon_element['src']  # to get the src attribute of the img tag\n",
    "    icon_list.append(icon_src)  # append url to list\n",
    "\n",
    "# check list\n",
    "print(icon_list)\n",
    "print(f'the most common weather condition is given by the icon: {mode(icon_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Devs_Institute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
